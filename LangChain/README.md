# LangChain + Ollama + Streamlit: Local RAG Example

This example demonstrates how to build a simple Retrieval-Augmented Generation (RAG) app using LangChain, Ollama (for LLM inference), and Streamlit (for the UI).

## Features
- Upload and index your own documents
- Ask questions and get answers grounded in your data
- All runs locally: no cloud LLMs required

## Files
- `rag_app.py`: Main Streamlit app
- `requirements.txt`: Python dependencies
- `README.md`: This guide

## Quickstart
1. Install Ollama and run a model (e.g., `ollama run llama3`)
2. Install requirements: `pip install -r requirements.txt`
3. Run the app: `streamlit run rag_app.py`

---

See `rag_app.py` for the full code and comments.
